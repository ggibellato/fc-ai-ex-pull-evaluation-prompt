###Pull, Otimização e Avaliação de Prompts com LangChain e LangSmith
Objetivo
Você deve entregar um software capaz de:

Fazer pull de prompts do LangSmith Prompt Hub contendo prompts de baixa qualidade
Refatorar e otimizar esses prompts usando técnicas avançadas de Prompt Engineering
Fazer push dos prompts otimizados de volta ao LangSmith
Avaliar a qualidade através de métricas customizadas (F1-Score, Clarity, Precision)
Atingir pontuação mínima de 0.9 (90%) em todas as métricas de avaliação
Exemplo no CLI
# Executar o pull dos prompts ruins do LangSmith
python src/pull_prompts.py

# Executar avaliação inicial (prompts ruins)
python src/evaluate.py

Executando avaliação dos prompts...
================================
Prompt: support_bot_v1a
- Helpfulness: 0.45
- Correctness: 0.52
- F1-Score: 0.48
- Clarity: 0.50
- Precision: 0.46
================================
Status: FALHOU - Métricas abaixo do mínimo de 0.9

# Após refatorar os prompts e fazer push
python src/push_prompts.py

# Executar avaliação final (prompts otimizados)
python src/evaluate.py

Executando avaliação dos prompts...
================================
Prompt: support_bot_v2_optimized
- Helpfulness: 0.94
- Correctness: 0.96
- F1-Score: 0.93
- Clarity: 0.95
- Precision: 0.92
================================
Status: APROVADO ✓ - Todas as métricas atingiram o mínimo de 0.9
Tecnologias obrigatórias
Linguagem: Python 3.9+
Framework: LangChain
Plataforma de avaliação: LangSmith
Gestão de prompts: LangSmith Prompt Hub
Formato de prompts: YAML
Pacotes recomendados
from langchain import hub  # Pull e Push de prompts
from langsmith import Client  # Interação com LangSmith API
from langsmith.evaluation import evaluate  # Avaliação de prompts
from langchain_openai import ChatOpenAI  # LLM OpenAI
from langchain_google_genai import ChatGoogleGenerativeAI  # LLM Gemini
OpenAI
Crie uma API Key da OpenAI: https://platform.openai.com/api-keys
Modelo de LLM para responder: gpt-4o-mini
Modelo de LLM para avaliação: gpt-4o
Custo estimado: ~$1-5 para completar o desafio
Gemini (modelo free)
Crie uma API Key da Google: https://aistudio.google.com/app/apikey
Modelo de LLM para responder: gemini-2.5-flash
Modelo de LLM para avaliação: gemini-2.5-flash
Limite: 15 req/min, 1500 req/dia
Requisitos
1. Pull dos Prompt inicial do LangSmith
O repositório base já contém prompts de baixa qualidade publicados no LangSmith Prompt Hub. Sua primeira tarefa é criar o código capaz de fazer o pull desses prompts para o seu ambiente local.

Tarefas:

Configurar suas credenciais do LangSmith no arquivo .env (conforme instruções no README.md do repositório base)
Acessar o script src/pull_prompts.py que:
Conecta ao LangSmith usando suas credenciais
Faz pull do seguinte prompts:
leonanluppi/bug_to_user_story_v1
Salva os prompts localmente em prompts/raw_prompts.yml
2. Otimização do Prompt
Agora que você tem o prompt inicial, é hora de refatorá-lo usando as técnicas de prompt aprendidas no curso.

Tarefas:

Analisar o prompt em prompts/bug_to_user_story_v1.yml
Criar um novo arquivo prompts/bug_to_user_story_v2.yml com suas versões otimizadas
Aplicar pelo menos duas das seguintes técnicas:
Few-shot Learning: Fornecer exemplos claros de entrada/saída
Chain of Thought (CoT): Instruir o modelo a "pensar passo a passo"
Tree of Thought: Explorar múltiplos caminhos de raciocínio
Skeleton of Thought: Estruturar a resposta em etapas claras
ReAct: Raciocínio + Ação para tarefas complexas
Role Prompting: Definir persona e contexto detalhado
Documentar no README.md quais técnicas você escolheu e por quê
Requisitos do prompt otimizado:

Deve conter instruções claras e específicas
Deve incluir regras explícitas de comportamento
Deve ter exemplos de entrada/saída (Few-shot)
Deve incluir tratamento de edge cases
Deve usar System vs User Prompt adequadamente
3. Push e Avaliação
Após refatorar os prompts, você deve enviá-los de volta ao LangSmith Prompt Hub.

Tarefas:

Criar o script src/push_prompts.py que:
<ul>
	<li>L&ecirc; os prompts otimizados de <code>prompts/bug_to_user_story_v2.yml</code></li>
	<li>Faz push para o LangSmith com nomes versionados:
	<ul>
		<li><code>{seu_username}/bug_to_user_story_v2</code></li>
	</ul>
	</li>
	<li>Adiciona metadados (tags, descri&ccedil;&atilde;o, t&eacute;cnicas utilizadas)</li>
</ul>
</li>
<li>Executar o script e verificar no dashboard do LangSmith se os prompts foram publicados</li>
<li>Deixa-lo p&uacute;blico</li>
4. Iteração
Espera-se 3-5 iterações.
Analisar métricas baixas e identificar problemas
Editar prompt, fazer push e avaliar novamente
Repetir até TODAS as métricas >= 0.9
Critério de Aprovação:
- Tone Score >= 0.9
- Acceptance Criteria Score >= 0.9
- User Story Format Score >= 0.9
- Completeness Score >= 0.9

MÉDIA das 4 métricas >= 0.9
IMPORTANTE: TODAS as 4 métricas devem estar >= 0.9, não apenas a média!

5. Testes de Validação
O que você deve fazer: Edite o arquivo tests/test_prompts.py e implemente, no mínimo, os 6 testes abaixo usando pytest:

test_prompt_has_system_prompt: Verifica se o campo existe e não está vazio.
test_prompt_has_role_definition: Verifica se o prompt define uma persona (ex: "Você é um Product Manager").
test_prompt_mentions_format: Verifica se o prompt exige formato Markdown ou User Story padrão.
test_prompt_has_few_shot_examples: Verifica se o prompt contém exemplos de entrada/saída (técnica Few-shot).
test_prompt_no_todos: Garante que você não esqueceu nenhum [TODO] no texto.
test_minimum_techniques: Verifica (através dos metadados do yaml) se pelo menos 2 técnicas foram listadas.
Como validar:

pytest tests/test_prompts.py
Estrutura obrigatória do projeto
Faça um fork do repositório base: [Clique aqui para o template](https://github.com/devfullcycle/mba-ia-pull-evaluation-prompt)

desafio-prompt-engineer/
├── .env.example              # Template das variáveis de ambiente
├── requirements.txt          # Dependências Python
├── README.md                 # Sua documentação do processo
│
├── prompts/
│   ├── bug_to_user_story_v1.yml       # Prompt inicial (após pull)
│   └── bug_to_user_story_v2.yml # Seu prompt otimizado
│
├── src/
│   ├── pull_prompts.py       # Pull do LangSmith
│   ├── push_prompts.py       # Push ao LangSmith
│   ├── evaluate.py           # Avaliação automática
│   ├── metrics.py            # 4 métricas implementadas
│   ├── dataset.py            # 15 exemplos de bugs
│   └── utils.py              # Funções auxiliares
│
├── tests/
│   └── test_prompts.py       # Testes de validação
│
O que você vai criar:

prompts/bug_to_user_story_v2.yml - Seu prompt otimizado
tests/test_prompts.py - Seus testes de validação
src/pull_prompt.py Script de pull do repositório da fullcycle
src/push_prompt.py Script de push para o seu repositório
README.md - Documentação do seu processo de otimização
O que já vem pronto:

Dataset com 15 bugs (5 simples, 7 médios, 3 complexos)
4 métricas específicas para Bug to User Story
Suporte multi-provider (OpenAI e Gemini)
Repositórios úteis
Repositório boilerplate do desafio
LangSmith Documentation
Prompt Engineering Guide
VirtualEnv para Python
Crie e ative um ambiente virtual antes de instalar dependências:

python3 -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
Ordem de execução
1. Executar pull dos prompts ruins
python src/pull_prompts.py
2. Refatorar prompts
Edite manualmente o arquivo prompts/bug_to_user_story_v2.yml aplicando as técnicas aprendidas no curso.

3. Fazer push dos prompts otimizados
python src/push_prompts.py
5. Executar avaliação
python src/evaluate.py
Entregável
Repositório público no GitHub (fork do repositório base) contendo:

<ul>
	<li>Todo o c&oacute;digo-fonte implementado</li>
	<li>Arquivo <code>prompts/bug_to_user_story_v2.yml</code> 100% preenchido e funcional</li>
	<li>Arquivo <code>README.md</code> atualizado com:</li>
</ul>
</li>
<li>
<p><strong>README.md deve conter:</strong></p>

<p>A) <strong>Se&ccedil;&atilde;o &quot;T&eacute;cnicas Aplicadas (Fase 2)&quot;</strong>:</p>

<ul>
	<li>Quais t&eacute;cnicas avan&ccedil;adas voc&ecirc; escolheu para refatorar os prompts</li>
	<li>Justificativa de por que escolheu cada t&eacute;cnica</li>
	<li>Exemplos pr&aacute;ticos de como aplicou cada t&eacute;cnica</li>
</ul>

<p>B) <strong>Se&ccedil;&atilde;o &quot;Resultados Finais&quot;</strong>:</p>

<ul>
	<li>Link p&uacute;blico do seu dashboard do LangSmith mostrando as avalia&ccedil;&otilde;es</li>
	<li>Screenshots das avalia&ccedil;&otilde;es com as notas m&iacute;nimas de 0.9 atingidas</li>
	<li>Tabela comparativa: prompts ruins (v1) vs prompts otimizados (v2)</li>
</ul>

<p>C) <strong>Se&ccedil;&atilde;o &quot;Como Executar&quot;</strong>:</p>

<ul>
	<li>Instru&ccedil;&otilde;es claras e detalhadas de como executar o projeto</li>
	<li>Pr&eacute;-requisitos e depend&ecirc;ncias</li>
	<li>Comandos para cada fase do projeto</li>
</ul>
</li>
<li>
<p><strong>Evid&ecirc;ncias no LangSmith</strong>:</p>

<ul>
	<li>Link p&uacute;blico (ou screenshots) do dashboard do LangSmith</li>
	<li>Devem estar vis&iacute;veis:
	<ul>
		<li>Dataset de avalia&ccedil;&atilde;o com &ge; 20 exemplos</li>
		<li>Execu&ccedil;&otilde;es dos prompts v1 (ruins) com notas baixas</li>
		<li>Execu&ccedil;&otilde;es dos prompts v2 (otimizados) com notas &ge; 0.9</li>
		<li>Tracing detalhado de pelo menos 3 exemplos</li>
	</ul>
	</li>
</ul>
</li>
Dicas Finais
Lembre-se da importância da especificidade, contexto e persona ao refatorar prompts
Use Few-shot Learning com 2-3 exemplos claros para melhorar drasticamente a performance
Chain of Thought (CoT) é excelente para tarefas que exigem raciocínio complexo (como análise de PRs)
Use o Tracing do LangSmith como sua principal ferramenta de debug - ele mostra exatamente o que o LLM está "pensando"
Não altere os datasets de avaliação
Itere, itere, itere - é normal precisar de 3-5 iterações para atingir 0.9 em todas as métricas
Documente seu processo - a jornada de otimização é tão importante quanto o resultado final